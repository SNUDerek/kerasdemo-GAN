{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-GAN with One-Hot Generator\n",
    "\n",
    "this is a demo of a GAN network modified prior distribution. instead of going from random noise to a random handwritted digit, this network is trained to take in a one-hot vector and generate a *corresponding* handwritten digit.\n",
    "\n",
    "main sources:\n",
    "\n",
    "- Generative Adversarial Text to Image Synthesis (paper)\n",
    "- https://arxiv.org/pdf/1605.05396.pdf\n",
    "- GAN by Example using Keras on Tensorflow Backend (blog article)\n",
    "- https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0\n",
    "- Keras-MNIST-GAN by Zackory (github)\n",
    "- https://github.com/Zackory/Keras-MNIST-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    " \n",
    " we will use the `tensorflow` MNIST data as it comes pre-normalized to `{0-1.0}`. \n",
    " \n",
    " also, we will take advantage of the keras `multi_gpu_model` to use dual-GPU training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must do this to tell keras how to handle image data\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the MNIST dataset\n",
    "\n",
    "the images will come as 28x28 scale images flattened to 784 with one channel (black & white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape - flattened 28x28 image\n",
    "28*28, np.shape(mnist.train.images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image data - do NOT reshape to 28x28 for feed-forward GAN\n",
    "image_train = mnist.train.images\n",
    "np.shape(image_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view one image\n",
    "# image_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label data (one-hot vectors as set above)\n",
    "label_train = mnist.train.labels\n",
    "label_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display image\n",
    "# https://stackoverflow.com/questions/42353676/display-mnist-image-using-matplotlib\n",
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(two_d, interpolation='nearest', cmap='gray')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at some sample target outputs\n",
    "\n",
    "we print out some test images for comparison with the generator. we want our generator output to look like this. notice the sharp edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the images match the labels (printed in the tuple)\n",
    "for i in range(5):\n",
    "    gen_image(mnist.test.images[i]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of the input - this is 10, one for each digit\n",
    "randomDim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network\n",
    "\n",
    "the generator is a dense network that takes in a vector (one-hot or one-hot with added noise) and learns to \"draw\" a handwrtitten digit by a series of `Dense` layers that output to a vector of size 784 = 28x28. we will create the network here, pretrain it, and observe the output. We have changed the network slightly from `Zackory`'s  implementation by using a final `sigmoid` activation. This seems reasonable as the `tensorflow` data is normalized between 0 and 1.0.\n",
    "\n",
    "notice that we are configured for two GPUs so the total batch size will be split equally between them. we make a template model (hence the \"t\") on the CPU and then assign to each GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    generatort = Sequential()\n",
    "    generatort.add(Dense(128, input_dim=randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02),\n",
    "                        name='gen_dns_1'))\n",
    "    generatort.add(Dense(256, activation='relu', name='gen_dns_2'))\n",
    "    generatort.add(Dense(512, activation='relu', name='gen_dns_3'))\n",
    "    generatort.add(Dense(1024, activation='relu', name='gen_dns_4'))\n",
    "    generatort.add(Dense(784, activation='sigmoid', name='gen_dns_5'))\n",
    "\n",
    "generator = multi_gpu_model(generatort, gpus=2)\n",
    "\n",
    "generator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the initialized weights to compare later\n",
    "init_gen_weights = generator.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (pre)-train the model for a few epochs\n",
    "\n",
    "we will train the network for a few epochs to see how it looks when trained in a traditional supervised method. this pre-training the model also helps the GAN train better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatort.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# supervised training\n",
    "generator.fit(label_train, image_train, epochs=1, verbose=0,\n",
    "             callbacks=[TQDMNotebookCallback(leave_inner=False, leave_outer=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretr_gen_weights = generator.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decode some examples to view them\n",
    "\n",
    "we will use the same first five test examples for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_in = mnist.test.labels[0:5]\n",
    "prd_out = generator.predict(prd_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial pretraining results\n",
    "\n",
    "we can see that the network did indeed learn quite a bit about the shape of the characters when trained with this supervised method. however the images don't resemble the targets above subjectively..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outs = []\n",
    "for i in range(len(prd_out)):\n",
    "    print(np.argmax(prd_in[i]))\n",
    "    gen_image(prd_out[i]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Discriminator Network\n",
    "\n",
    "the discriminator learns to differentiate between the generator outputs and the true images. for example, from the examples above, ideally the discriminator would learn that those blurry edges are poor features (not exactly, but its a good visual analogy).\n",
    "\n",
    "because we are also trying to create a generator that goes from the desired number to a good image, we should also give the discriminator the original one-hot input along with the image. we should also train it with some negative matches from the true data (and the generated data?) to help it learn this task as well.\n",
    "\n",
    "due to the multi-input nature, we will use the keras functional API to make this model. The image will go through a series of feed-forward layers, it will be concatenated with the input, and then this vector will output as 0 (Fake/Bad) or 1 (Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    # image \"reader\" as series of dense layers\n",
    "    img_in = Input(shape=(784, ), name='img_in')\n",
    "    img = Dense(784, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=0.02),\n",
    "                name='img_dns_1')(img_in)\n",
    "    img = Dropout(0.2, name='img_drp_1')(img)\n",
    "    img = Dense(512, activation='relu', name='img_dns_2')(img)\n",
    "    img = Dropout(0.2, name='img_drp_2')(img)\n",
    "    img = Dense(384, activation='relu', name='img_dns_3')(img)\n",
    "    img = Dropout(0.2, name='img_drp_3')(img)\n",
    "    img = Dense(128, activation='relu', name='img_dns_4')(img)\n",
    "    \n",
    "    # one-hot is just added by concatenation\n",
    "    txt_in = Input(shape=(randomDim, ), name='txt_in')\n",
    "    cmb = concatenate([img, txt_in])\n",
    "    \n",
    "    # final dense layers\n",
    "    cmb = Dense(64, activation='relu', name='out_dns_1')(cmb)\n",
    "    out = Dense(1, activation='sigmoid', name='out_dns_2')(cmb)\n",
    "    \n",
    "    discriminatort = Model(inputs=[img_in, txt_in], outputs=[out])\n",
    "    \n",
    "discriminator = multi_gpu_model(discriminatort, gpus=2)\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "discriminatort.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_disc_weights = discriminator.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discriminator training data\n",
    "\n",
    "this needs at least three sources:\n",
    "\n",
    "- generator input, true label: FALSE\n",
    "- true image, true label: TRUE\n",
    "- true image, false label: FALSE\n",
    "\n",
    "[Maybe?] these should also be adjusted so the TRUE/FALSE labels are 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator output : predicting on the labels\n",
    "image_train_gen = generator.predict(label_train)\n",
    "label_train_gen = label_train[:]\n",
    "value_train_gen = np.zeros(image_train_gen.shape[0]) # FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true labels are just that\n",
    "image_train_tru = image_train[:]\n",
    "label_train_tru = label_train[:]\n",
    "value_train_tru = np.ones(image_train_tru.shape[0]) # TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both bad labels need to be constructed\n",
    "# shift vectors with np.roll\n",
    "def rollshuffle(vectors):\n",
    "    rndroll = np.random.randint(1, len(vectors))\n",
    "    vectors = np.roll(vectors, rndroll, axis=1) \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(len(data), data)\n",
    "data = to_categorical(data, num_classes=randomDim)\n",
    "data = rollshuffle(data)\n",
    "data = np.argmax(data, axis=1)\n",
    "print(len(data), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true images with bad labels\n",
    "image_train_tru_bad = image_train[:]\n",
    "label_train_tru_bad = rollshuffle(label_train)\n",
    "value_train_tru_bad = np.zeros(image_train_tru_bad.shape[0]) # FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "label_train[0], label_train_tru_bad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator with bad labels\n",
    "image_train_gen_bad = image_train_gen[:]\n",
    "label_train_gen_bad = rollshuffle(label_train)\n",
    "value_train_gen_bad = np.zeros(image_train_gen_bad.shape[0]) # FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "label_train[0], label_train_gen_bad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the images, labels and values (good, bad)\n",
    "image_train_all = np.concatenate((image_train_gen,\n",
    "                                  image_train_tru, \n",
    "                                  image_train_tru_bad,\n",
    "                                  # image_train_gen_bad\n",
    "                                 ), axis=0)\n",
    "label_train_all = np.concatenate((label_train_gen,\n",
    "                                  label_train_tru, \n",
    "                                  label_train_tru_bad,\n",
    "                                  # label_train_gen_bad\n",
    "                                 ), axis=0)\n",
    "value_train_all = np.concatenate((value_train_gen,\n",
    "                                  value_train_tru, \n",
    "                                  value_train_tru_bad,\n",
    "                                  # value_train_gen_bad\n",
    "                                 ), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "def shufflelist(x, y, z):\n",
    "    assert(x.shape[0] == y.shape[0])\n",
    "    assert(y.shape[0] == z.shape[0])\n",
    "    idx = np.random.permutation(x.shape[0])\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    z = z[idx]\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_all, label_train_all, value_train_all = shufflelist(image_train_all, \n",
    "                                                                label_train_all,\n",
    "                                                                value_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-training\n",
    "discriminator.fit([image_train_all, label_train_all], [value_train_all], \n",
    "                  epochs=1, verbose=0,\n",
    "                  callbacks=[TQDMNotebookCallback(leave_inner=False, leave_outer=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretr_disc_weights = discriminator.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test discriminator\n",
    "\n",
    "decode on some `test` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_labels = mnist.test.labels[0:3]\n",
    "bad_labels = rollshuffle(tst_labels)\n",
    "tst_images = mnist.test.images[0:3]\n",
    "gen_images = generator.predict(tst_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test various combinations if you want, but importantly these should be FALSE\n",
    "# this shows that the discriminator can identify generated images vs trues\n",
    "prd_outs = discriminator.predict([gen_images, tst_labels])\n",
    "outs = []\n",
    "for i in prd_outs:\n",
    "    if i > 0.5:\n",
    "        print(i, 'TRUE')\n",
    "    else:\n",
    "        print(i, \"FALSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data-generation function\n",
    "\n",
    "add the above discriminator dataset generation code to a function for easy calling in GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdiscriminatordata(labelBatch, imageBatch, imageGen, batchSize):\n",
    "    # generator output : predicting on the labels\n",
    "    image_train_gen = imageGen[:]\n",
    "    label_train_gen = labelBatch[:]\n",
    "    value_train_gen = np.zeros(image_train_gen.shape[0]) # FALSE\n",
    "    # true labels are just that\n",
    "    image_train_tru = imageBatch[:]\n",
    "    label_train_tru = labelBatch[:]\n",
    "    value_train_tru = np.ones(image_train_tru.shape[0]) # TRUE\n",
    "    # true images with bad labels\n",
    "    image_train_tru_bad = imageBatch[:]\n",
    "    label_train_tru_bad = rollshuffle(labelBatch)\n",
    "    value_train_tru_bad = np.zeros(image_train_tru_bad.shape[0]) # FALSE\n",
    "    # generator with bad labels\n",
    "    image_train_gen_bad = image_train_gen[:]\n",
    "    label_train_gen_bad = rollshuffle(labelBatch)\n",
    "    value_train_gen_bad = np.zeros(image_train_gen_bad.shape[0]) # FALSE\n",
    "    # combine all the images, labels and values (good, bad)\n",
    "    image_train_all = np.concatenate((image_train_gen,\n",
    "                                      image_train_tru, \n",
    "                                      image_train_tru_bad,\n",
    "                                      image_train_gen_bad\n",
    "                                     ), axis=0)\n",
    "    label_train_all = np.concatenate((label_train_gen,\n",
    "                                      label_train_tru, \n",
    "                                      label_train_tru_bad,\n",
    "                                      label_train_gen_bad\n",
    "                                     ), axis=0)\n",
    "    value_train_all = np.concatenate((value_train_gen,\n",
    "                                      value_train_tru, \n",
    "                                      value_train_tru_bad,\n",
    "                                      value_train_gen_bad\n",
    "                                     ), axis=0)\n",
    "    \n",
    "    image_train_all, label_train_all, value_train_all = shufflelist(image_train_all, \n",
    "                                                                label_train_all,\n",
    "                                                                value_train_all)\n",
    "    \n",
    "    return image_train_all, label_train_all, value_train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN network\n",
    "\n",
    "we make some modifications by outputting both the generator network output and the discriminator output, and training the network on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined network\n",
    "discriminator.trainable = False\n",
    "\n",
    "# the input to the generator\n",
    "# the generator output is considered as one terminal layer\n",
    "# and loss calculated by difference from TRUE image\n",
    "genInput = Input(shape=(randomDim,), name='gen_in')\n",
    "genOutput = generator(genInput)\n",
    "\n",
    "# the discriminator takes the generator output\n",
    "dscInput = Input(shape=(784,), name='dsc_in')\n",
    "dscOutput = discriminator([genOutput, genInput])\n",
    "\n",
    "# here we make the final joint model\n",
    "# we weight the generator output more than the GAN output\n",
    "gan = Model(inputs=[genInput, dscInput], outputs=[genOutput, dscOutput])\n",
    "gan.compile(loss='binary_crossentropy', loss_weights=[0.0, 1.0], optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gantrain(epochs=15, batchSize=64):\n",
    "    \n",
    "    batchCount = int(label_train.shape[0] / batchSize)\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', batchSize)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        # print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        # print out images per x epochs\n",
    "        # if e % 5 == 0:\n",
    "        prd_in = mnist.test.labels[0:5]\n",
    "        prd_out = generator.predict(prd_in)\n",
    "        i = 1\n",
    "        gen_image(prd_out[i]).show()\n",
    "        \n",
    "        for _ in tqdm(range(batchCount)):\n",
    "            \n",
    "            # get a random set of inputs and images\n",
    "            labelBatch = label_train[np.random.randint(0, label_train.shape[0], size=batchSize)]\n",
    "            imageBatch = image_train[np.random.randint(0, image_train.shape[0], size=batchSize)]\n",
    "            \n",
    "            # generate images from the inputs\n",
    "            imageGen = generator.predict(labelBatch)\n",
    "            \n",
    "            # generate full training data for discriminator\n",
    "            X1, X2, yDis = getdiscriminatordata(labelBatch, imageBatch, imageGen, batchSize)\n",
    "            \n",
    "            # train discriminator with this fuller data\n",
    "            discriminator.trainable = True\n",
    "            dloss = discriminator.train_on_batch([X1, X2], yDis)\n",
    "\n",
    "            # train generator\n",
    "            yGen = np.ones(batchSize)\n",
    "            discriminator.trainable = False\n",
    "            # print(labelBatch.shape, imageBatch.shape, yGen.shape)\n",
    "            gloss = gan.train_on_batch([labelBatch, imageBatch], [imageBatch, yGen])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if retraining over and over, reset weights\n",
    "generator.set_weights(pretr_gen_weights)\n",
    "discriminator.set_weights(pretr_disc_weights)\n",
    "gantrain(20, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prd_in = mnist.test.labels[0:5]\n",
    "tru_out = mnist.test.images[0:5]\n",
    "prd_out = generator.predict(prd_in)\n",
    "\n",
    "outs = []\n",
    "for i in range(len(prd_out)):\n",
    "    print(np.argmax(prd_in[i]))\n",
    "    gen_image(prd_out[i]).show()\n",
    "    gen_image(tru_out[i]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline generator comparison\n",
    "\n",
    "the generator model was trained for a total of 7 epochs (2 pre-training and 5 in adverserial). so we can reset the weights and train for 20 epochs in normal supervised to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset with the initial saved state\n",
    "generator.set_weights(init_gen_weights)\n",
    "# supervised training\n",
    "generator.fit(label_train, image_train, epochs=7, verbose=0,\n",
    "             callbacks=[TQDMNotebookCallback(leave_inner=False, leave_outer=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prd_in = mnist.test.labels[0:5]\n",
    "tru_out = mnist.test.images[0:5]\n",
    "prd_out = generator.predict(prd_in)\n",
    "\n",
    "outs = []\n",
    "for i in range(len(prd_out)):\n",
    "    print(np.argmax(prd_in[i]))\n",
    "    gen_image(prd_out[i]).show()\n",
    "    gen_image(tru_out[i]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
